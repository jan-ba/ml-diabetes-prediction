{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data and using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a pandas dataframe\n",
    "df = pd.read_csv(\"diabetes_data.csv\")\n",
    "print(df.size)\n",
    "\n",
    "# do some basic exploration\n",
    "print(\"(Observations, Attributes (including target): \", df.shape)\n",
    "print(\"DF contains null / nan values: \", df.isna().any().any())\n",
    "for i in df.columns:\n",
    "    print(f\"Unique values for {i}: \", sorted(df[i].unique()))\n",
    "    # plt.figure(figsize=(3, 1.5))\n",
    "    # plt.title(f'{i}')\n",
    "    # plt.hist(df[i])\n",
    "print(df.describe())\n",
    "\n",
    "# (for our report, we should describe how the categories were assessed (e.g. age buckets, what the 5 GenHlth categories correspond to etc.))\n",
    "\n",
    "# As can be seen, many categories are actually binary, the rest is somewhat categorical although interpreting them continously might bear some benefit\n",
    "# One hot encoding, dropfirst to remove the redundant column, since for sex they will have perfect negative corrolation (e.g. is_female and is_male)\n",
    "df = pd.get_dummies(df, columns=[\"Sex\"], drop_first=True) \n",
    "# One hot encoding for GeneralHealth since its a catagorical value from 1-5\n",
    "# df = pd.get_dummies(df, columns=[\"GenHlth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for cyclical values and if we need to scale some features (maybe its better to leave some features unscaled for some models ???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if we have cyclical values | We dont :) often in form of months, days or any time series.\n",
    "\n",
    "# Check scaling and what columns its need to do scaling (essentially if range is above 0<x<1 => we need to scale)\n",
    "# our exploration above yields: Age, BMI, (GenHlth, now one-hot encoded,) MentHlth, PhysHlth\n",
    "# Init of scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling for BMI, Mental health, Physical Health, and maybee age and GenHlth (depending on encoded or not) ??? \n",
    "# to benefit models like logistic regression, SVM or KNN\n",
    "to_be_scaled = [\"BMI\", \"MentHlth\", \"PhysHlth\", \"Age\", \"GenHlth\"]\n",
    "df[to_be_scaled] = scaler.fit_transform(df[to_be_scaled])\n",
    "# print(df.describe())\n",
    "\n",
    "# Splitting the data into data and target\n",
    "data = df.drop(\"Diabetes\", axis=1)\n",
    "target = df[\"Diabetes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into triaining and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a train and test split\n",
    "# Train 80%\n",
    "# Test 20%\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, stratify=target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for correlation of features in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check features that are correlated\n",
    "plt.figure(figsize=(15, 15))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.plot()\n",
    "# there seems to be a slight linear correlation between GenHtlh and PhysHlth, we should address this / state what the threshold value would be and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pycaret to choose top 5 models to train develope further with hyperparameters optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train.copy()\n",
    "train_data[\"Diabetes\"] = y_train\n",
    "\n",
    "# Pycaret for choosing the best models, pick top 5.\n",
    "s = ClassificationExperiment()\n",
    "s.setup(train_data, target = \"Diabetes\", session_id = 123, preprocess=True)  #preprocessing probably also takes care of scaling e.g.\n",
    "top_models = s.compare_models(n_select=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter-tuning: random? bayesian optimisation? grid-search takes a lot of time but is precise\n",
    "# Libs like\n",
    "    # Optuna\n",
    "    # Hyper-opt\n",
    "\n",
    "# built-in grid-search by pycaret (took 2m 2.1s for me)\n",
    "tuned_models = [s.tune_model(model) for model in top_models]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_RU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
